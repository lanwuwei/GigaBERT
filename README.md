## GigaBERT
This repo contains pre-trained models for [GigaBERT](https://arxiv.org/pdf/2004.14519.pdf):

	@inproceedings{lan2020gigabert,
	  author     = {Lan, Wuwei and Chen, Yang and Xu, Wei and Ritter, Alan},
  	  title      = {GigaBERT: Zero-shot Transfer Learning from English to Arabic},
  	  booktitle  = {Proceedings of The 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP)},
  	  year       = {2020}
  	} 

## Fine-tuning Experiments
Please [Yang Chen](https://edchengg.github.io/)'s GitHub for code and data.

Navigate to the task fodler and check run.sh for sample command. 

Modify the file path for configuration file, model checkpoint and vocabulary in main.py.

## Checkpoints
The pre-trained models can be found here: [GigaBERT-v3](https://drive.google.com/drive/folders/1zgUXz8FQPHmWVNR7tHyPq1E6SmrMuPv6?usp=sharing) and [GigaBERT-v4](https://drive.google.com/drive/folders/1uFGzMuTOD7iNsmKQYp_zVuvsJwOaIdar?usp=sharing) 

Please contact [Wuwei Lan](lan.105@osu.edu) for the original TF checkpoints or code-switched GigaBERT in different versions.

